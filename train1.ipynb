{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
    "from model.LPRNet import build_lprnet\n",
    "# import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from detection import utils\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "global Greedy_Decode_Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_for_ctc(T_length, lengths):\n",
    "    input_lengths = []\n",
    "    target_lengths = []\n",
    "\n",
    "    for ch in lengths:\n",
    "        input_lengths.append(T_length)\n",
    "        target_lengths.append(ch)\n",
    "\n",
    "    return tuple(input_lengths), tuple(target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, cur_epoch, base_lr, lr_schedule):\n",
    "    \"\"\"\n",
    "    Sets the learning rate\n",
    "    \"\"\"\n",
    "    lr = 0\n",
    "    for i, e in enumerate(lr_schedule):\n",
    "        if cur_epoch < e:\n",
    "            lr = base_lr * (0.1 ** i)\n",
    "            break\n",
    "    if lr == 0:\n",
    "        lr = base_lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='parameters to train net')\n",
    "    parser.add_argument('--max_epoch', default=15, help='epoch to train the network')\n",
    "    parser.add_argument('--img_size', default=[94, 24], help='the image size')\n",
    "    parser.add_argument('--train_img_dirs', default=\"data/train\", help='the train images path')\n",
    "    parser.add_argument('--test_img_dirs', default=\"~/workspace/testMixLPR\", help='the test images path')\n",
    "    parser.add_argument('--dropout_rate', default=0.5, help='dropout rate.')\n",
    "    parser.add_argument('--learning_rate', default=0.1, help='base value of learning rate.')\n",
    "    parser.add_argument('--lpr_max_len', default=8, help='license plate number max length.')\n",
    "    parser.add_argument('--train_batch_size', default=128, help='training batch size.')\n",
    "    parser.add_argument('--test_batch_size', default=120, help='testing batch size.')\n",
    "    parser.add_argument('--phase_train', default=True, type=bool, help='train or test phase flag.')\n",
    "    parser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\n",
    "    parser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\n",
    "    parser.add_argument('--resume_epoch', default=0, type=int, help='resume iter for retraining')\n",
    "    parser.add_argument('--save_interval', default=2000, type=int, help='interval for save model state dict')\n",
    "    parser.add_argument('--test_interval', default=2000, type=int, help='interval for evaluate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "    parser.add_argument('--weight_decay', default=2e-5, type=float, help='Weight decay for SGD')\n",
    "    parser.add_argument('--lr_schedule', default=[4, 8, 12, 14, 16], help='schedule for learning rate.')\n",
    "    parser.add_argument('--save_folder', default='./weights/', help='Location to save checkpoint models')\n",
    "    # parser.add_argument('--pretrained_model', default='./weights/Final_LPRNet_model.pth', help='pretrained base model')\n",
    "    parser.add_argument('--pretrained_model', default='', help='pretrained base model')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        img, label, length = sample\n",
    "        imgs.append(torch.from_numpy(img))\n",
    "        labels.extend(label)\n",
    "        lengths.append(length)\n",
    "    labels = np.asarray(labels).flatten().astype(np.int)\n",
    "\n",
    "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    args = get_parser()\n",
    "\n",
    "    T_length = 18 # args.lpr_max_len\n",
    "    epoch = 0 + args.resume_epoch\n",
    "    loss_val = 0\n",
    "\n",
    "    if not os.path.exists(args.save_folder):\n",
    "        os.mkdir(args.save_folder)\n",
    "\n",
    "    lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lprnet.to(device)\n",
    "    print(\"Successful to build network!\")\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained_model:\n",
    "        lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
    "        print(\"load pretrained model successful!\")\n",
    "    else:\n",
    "        def xavier(param):\n",
    "            nn.init.xavier_uniform(param)\n",
    "\n",
    "        def weights_init(m):\n",
    "            for key in m.state_dict():\n",
    "                if key.split('.')[-1] == 'weight':\n",
    "                    if 'conv' in key:\n",
    "                        nn.init.kaiming_normal_(m.state_dict()[key], mode='fan_out')\n",
    "                    if 'bn' in key:\n",
    "                        m.state_dict()[key][...] = xavier(1)\n",
    "                elif key.split('.')[-1] == 'bias':\n",
    "                    m.state_dict()[key][...] = 0.01\n",
    "\n",
    "        lprnet.backbone.apply(weights_init)\n",
    "        lprnet.container.apply(weights_init)\n",
    "        print(\"initial net weights successful!\")\n",
    "\n",
    "    # define optimizer\n",
    "    # optimizer = optim.SGD(lprnet.parameters(), lr=args.learning_rate,\n",
    "    #                       momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    optimizer = optim.RMSprop(lprnet.parameters(), lr=args.learning_rate, alpha = 0.9, eps=1e-08,\n",
    "                         momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    train_img_dirs = os.path.expanduser(args.train_img_dirs)\n",
    "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
    "    train_dataset = LPRDataLoader(train_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "\n",
    "    epoch_size = len(train_dataset) // args.train_batch_size\n",
    "    max_iter = args.max_epoch * epoch_size\n",
    "\n",
    "    ctc_loss = nn.CTCLoss(blank=len(CHARS)-1, reduction='mean') # reduction: 'none' | 'mean' | 'sum'\n",
    "\n",
    "    if args.resume_epoch > 0:\n",
    "        start_iter = args.resume_epoch * epoch_size\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        if iteration % epoch_size == 0:\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(DataLoader(train_dataset, args.train_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
    "            loss_val = 0\n",
    "            epoch += 1\n",
    "\n",
    "        if iteration !=0 and iteration % args.save_interval == 0:\n",
    "            torch.save(lprnet.state_dict(), args.save_folder + 'LPRNet_' + '_iteration_' + repr(iteration) + '.pth')\n",
    "\n",
    "        if (iteration + 1) % args.test_interval == 0:\n",
    "            Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
    "            # lprnet.train() # should be switch to train mode\n",
    "\n",
    "        start_time = time.time()\n",
    "        # load train data\n",
    "        images, labels, lengths = next(batch_iterator)\n",
    "        # labels = np.array([el.numpy() for el in labels]).T\n",
    "        # print(labels)\n",
    "        # get ctc parameters\n",
    "        input_lengths, target_lengths = sparse_tuple_for_ctc(T_length, lengths)\n",
    "        # update lr\n",
    "        lr = adjust_learning_rate(optimizer, epoch, args.learning_rate, args.lr_schedule)\n",
    "\n",
    "        if args.cuda:\n",
    "            images = Variable(images, requires_grad=False).cuda()\n",
    "            labels = Variable(labels, requires_grad=False).cuda()\n",
    "        else:\n",
    "            images = Variable(images, requires_grad=False)\n",
    "            labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "        # forward\n",
    "        logits = lprnet(images)\n",
    "        log_probs = logits.permute(2, 0, 1) # for ctc loss: T x N x C\n",
    "        # print(labels.shape)\n",
    "        log_probs = log_probs.log_softmax(2).requires_grad_()\n",
    "        # log_probs = log_probs.detach().requires_grad_()\n",
    "        # print(log_probs.shape)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss = ctc_loss(log_probs, labels, input_lengths=input_lengths, target_lengths=target_lengths)\n",
    "        if loss.item() == np.inf:\n",
    "            continue\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "        end_time = time.time()\n",
    "        if iteration % 20 == 0:\n",
    "            print('Epoch:' + repr(epoch) + ' || epochiter: ' + repr(iteration % epoch_size) + '/' + repr(epoch_size)\n",
    "                  + '|| Totel iter ' + repr(iteration) + ' || Loss: %.4f||' % (loss.item()) +\n",
    "                  'Batch time: %.4f sec. ||' % (end_time - start_time) + 'LR: %.8f' % (lr))\n",
    "    # final test\n",
    "    print(\"Final test Accuracy:\")\n",
    "    Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
    "\n",
    "    # save final parameters\n",
    "    torch.save(lprnet.state_dict(), args.save_folder + 'Final_LPRNet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Greedy_Decode_Eval(Net, datasets, args):\n",
    "    # TestNet = Net.eval()\n",
    "    epoch_size = len(datasets) // args.test_batch_size\n",
    "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
    "\n",
    "    Tp = 0\n",
    "    Tn_1 = 0\n",
    "    Tn_2 = 0\n",
    "    t1 = time.time()\n",
    "    for i in range(epoch_size):\n",
    "        # load train data\n",
    "        images, labels, lengths = next(batch_iterator)\n",
    "        start = 0\n",
    "        targets = []\n",
    "        for length in lengths:\n",
    "            label = labels[start:start+length]\n",
    "            targets.append(label)\n",
    "            start += length\n",
    "        targets = np.array([el.numpy() for el in targets])\n",
    "\n",
    "        if args.cuda:\n",
    "            images = Variable(images.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "\n",
    "        # forward\n",
    "        prebs = Net(images)\n",
    "        # greedy decode\n",
    "        prebs = prebs.cpu().detach().numpy()\n",
    "        preb_labels = list()\n",
    "        for i in range(prebs.shape[0]):\n",
    "            preb = prebs[i, :, :]\n",
    "            preb_label = list()\n",
    "            for j in range(preb.shape[1]):\n",
    "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
    "            no_repeat_blank_label = list()\n",
    "            pre_c = preb_label[0]\n",
    "            if pre_c != len(CHARS) - 1:\n",
    "                no_repeat_blank_label.append(pre_c)\n",
    "            for c in preb_label: # dropout repeate label and blank label\n",
    "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
    "                    if c == len(CHARS) - 1:\n",
    "                        pre_c = c\n",
    "                    continue\n",
    "                no_repeat_blank_label.append(c)\n",
    "                pre_c = c\n",
    "            preb_labels.append(no_repeat_blank_label)\n",
    "        for i, label in enumerate(preb_labels):\n",
    "            if len(label) != len(targets[i]):\n",
    "                Tn_1 += 1\n",
    "                continue\n",
    "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
    "                Tp += 1\n",
    "            else:\n",
    "                Tn_2 += 1\n",
    "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
    "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
    "    t2 = time.time()\n",
    "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f2c22dc2d842368364640f9b6cb66d0973a4d608cdd090292daee23382d5e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
